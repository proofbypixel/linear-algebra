<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proof by Pixel</title>

    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script>
        function togglePost(id) {
            var fullContent = document.getElementById(id + '-full');
            var summary = document.getElementById(id + '-summary');
            
            if (fullContent.style.display === "none") {
                fullContent.style.display = "block";
                summary.style.display = "none";
                // Re-render math in case it was hidden
                if (window.MathJax) MathJax.typesetPromise(); 
            } else {
                fullContent.style.display = "none";
                summary.style.display = "block";
            }
        }
    </script>

    <style>
        /* --- GLOBAL RESET --- */
        * { box-sizing: border-box; } 
        body { margin: 0; padding: 0; background-color: #f6f6f6; font-family: "Georgia", "Times New Roman", serif; color: #333; line-height: 1.6; font-size: 16px; overflow-x: hidden; }
        a { color: #0066cc; text-decoration: none; cursor: pointer; }
        a:hover { text-decoration: underline; }

        /* --- LAYOUT --- */
        .wrapper { max-width: 960px; width: 96%; margin: 0 auto; background-color: #ffffff; border-left: 1px solid #ddd; border-right: 1px solid #ddd; min-height: 100vh; }
        header { padding: 40px 40px 20px 40px; background-color: #ffffff; border-bottom: 1px solid #eee; }
        header h1 { margin: 0; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px; letter-spacing: -1px; }
        .tagline { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 14px; color: #888; font-style: italic; }

        nav { padding: 15px 40px; background-color: #fafafa; border-bottom: 1px solid #eee; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 13px; font-weight: bold; text-transform: uppercase; }
        nav a { margin-right: 25px; color: #444; }
        nav a:hover { color: #0066cc; }

        .container { display: flex; padding: 30px 40px; }
        .content { flex: 2; padding-right: 40px; min-width: 0; } 
        .sidebar { flex: 1; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 12px; color: #666; border-left: 1px solid #eee; padding-left: 20px; min-width: 0; }

        /* --- POST STYLING --- */
        .post { margin-bottom: 40px; border-bottom: 1px solid #eee; padding-bottom: 40px; }
        .post:last-child { border-bottom: none; }
        
        .post-title { 
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; 
            font-size: 24px; 
            font-weight: bold; 
            margin-bottom: 5px; 
            color: #000; 
            cursor: pointer; 
        }
        .post-title:hover { color: #0066cc; }

        .post-date { font-size: 11px; color: #888; margin-bottom: 15px; text-transform: uppercase; letter-spacing: 1px; }
        
        /* Summary View */
        .post-summary p { font-size: 16px; color: #555; margin-bottom: 10px; }
        .read-more { 
            display: inline-block; 
            margin-top: 10px; 
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; 
            font-size: 13px; 
            font-weight: bold; 
            color: #0066cc; 
            cursor: pointer;
            border: 1px solid #0066cc;
            padding: 5px 10px;
            border-radius: 3px;
        }
        .read-more:hover { background: #0066cc; color: white; text-decoration: none; }

        /* Content Styling */
        h3 { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 18px; margin-top: 30px; border-bottom: 2px solid #333; padding-bottom: 5px; }
        h4 { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 15px; margin-top: 25px; font-weight: bold; color: #444; }
        p { margin-bottom: 1.2em; text-align: left; }
        pre { background:#f4f4f4; padding:15px; border-radius:5px; font-family: "Monaco", "Courier New", monospace; font-size:13px; overflow-x: auto; border: 1px solid #eee; border-left: 3px solid #333; }
        
        /* Hidden by default */
        .post-full { display: none; animation: fadeIn 0.5s; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }

        /* Sidebar Widgets */
        .widget { margin-bottom: 40px; }
        .widget h3 { font-size: 11px; text-transform: uppercase; color: #333; border-bottom: 1px solid #ddd; padding-bottom: 5px; margin-bottom: 10px; margin-top: 0; }
        .widget ul { list-style: none; padding: 0; margin: 0; }
        .widget li { margin-bottom: 8px; }

        @media (max-width: 768px) {
            .container { flex-direction: column; padding: 20px; }
            .sidebar { border-left: none; padding-left: 0; margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }
            .content { padding-right: 0; }
            nav { padding: 15px 20px; }
        }
    </style>
</head>
<body>

<div class="wrapper">
    <header>
        <h1>Proof by Pixel</h1>
        <div class="tagline">A Data Science Blog: Learning & proving concepts one pixel at a time.</div>
    </header>

    <nav>
        <a href="#">Home</a>
        <a href="#">About Us</a>
        <a href="#">Level 1</a>
        <a href="#">Level 2</a>
    </nav>

    <div class="container">
        <div class="content">

            <div class="post" id="post-002">
                
                <div class="post-title" onclick="togglePost('post-002')">Log 002: On Markov Chains</div>
                <div class="post-date">21 December 2025</div>
                
                <div class="post-summary" id="post-002-summary">
                    <p>We take a physical system $S$ which can occupy distinct states like $e_1, e_2, \dots$. The rule for movement is simple but powerful: The system has no memory.</p>
                    <div class="read-more" onclick="togglePost('post-002')">Read Log 002 &rarr;</div>
                </div>

                <div class="post-full" id="post-002-full">
                    
                    <p><em>I was reading Markov Chains from the book Probability Theory: A Concise Course by Y.A. Rozanov.</em></p>

                    <p><strong>1. Introduction:</strong><br>
                    Here we take a physical system $S$ which can occupy distinct states like $e_1, e_2, \dots$. Starting from an initial state when time $t=0$, the system moves from one state to another at each tick of the clock. This collection of all possible states is called the <strong>State Space</strong>.</p>

                    <p><strong>2. The Rule (No Memory):</strong><br>
                    The rule for movement is: The system has no memory. The probability of the system occupying state $j$ at the next time step $n+1$, given the entire history of previous states $X_0$ to $X_n$, is equal to the probability dependent <em>only</em> on the current state $X_n$.
                    $$P(X_{n+1} = j | X_0, \dots, X_n = i) = P(X_{n+1} = j | X_n = i)$$
                    This specific kind of memoryless randomness is called the <strong>Markov Property</strong>.</p>

                    <pre>
# Python Concept:
# The next state depends ONLY on 'current_state'.
def get_next_state(current_state, history):
    return random_choice(transitions[current_state])
</pre>

                    <p><strong>3. The Matrix (Data Science Application):</strong><br>
                    In Data Science, we don't just see this as abstract math; we see it as a transition map. Imagine modeling <strong>Customer Churn</strong>. State 1 is "Subscribed" and State 2 is "Unsubscribed". The matrix $P$ holds the probabilities of a user switching between these states next month.
                    $$\sum_{j} p_{ij} = 1$$
                    </p>

                    <pre>
import numpy as np
# A Churn Matrix: Row 0 (Active), Row 1 (Churned)
P = np.array([[0.90, 0.10], [0.05, 0.95]])
print(P.sum(axis=1)) # Output: [1. 1.]
</pre>

                    <br><hr><br>

                    <h3>II. Here's where it got very interesting</h3>
                    <p>Predicting the future... If $P$ tells us where we go in one step, can we predict where we will be in the distant future?</p>

                    <p>Suppose we have already taken $n$ steps (let's say $n=2$ days). We want to know the probability of moving from state $i$ to state $j$ after <em>another</em> $m$ steps (say $m=3$ more days). The logic is simple: To get from $i$ to $j$ in total time ($n+m$), we just multiply the probability of the first leg of the trip ($n$) by the probability of the second leg ($m$).
                    $$p_{ij}^{(n+m)} = \sum_{k} p_{ik}^{(n)} p_{kj}^{(m)}$$
                    These are the famous <strong>Chapman-Kolmogorov Equations</strong>.</p>

                    <pre>
# Predicting 5 days into the future (n=2 + m=3)
P_2 = np.linalg.matrix_power(P, 2)
P_3 = np.linalg.matrix_power(P, 3)
P_5 = np.dot(P_2, P_3)
</pre>
                    <br><hr><br>
                    
                    <p><em><strong>Note:</strong> I also studied Recurrent and Transient states, a bunch of theorems related to accessibility, and Limiting Probabilities. However, those proofs are dense enough to warrant their own post. I will save them for a different log some other time.</em></p>

                    <div class="read-more" onclick="togglePost('post-002')">&uarr; Close Log</div>
                </div>
            </div>
            
            
            <div class="post" id="post-001">
                
                <div class="post" id="post-001">
                
                <div class="post-title" onclick="togglePost('post-001')">Log 001: Understanding the Basis of Data</div>
                <div class="post-date">16 December 2025</div> 
                
                <div class="post-summary" id="post-001-summary">
                    <p><strong>Topic:</strong> Vector Spaces, Span & Linear Independence<br>
                    <strong>Application:</strong> Feature Engineering (Multicollinearity)<br>
                    <strong>The Result:</strong> Using math to identify redundant data features.</p>
                    <div class="read-more" onclick="togglePost('post-001')">Open Log &rarr;</div>
                </div>

                <div class="post-full" id="post-001-full">
                    
                    <div class="study-log">
                        <h4 style="margin-top:0;">üìù The Study Log</h4>
                        <ul>
                            <li><strong>Topics:</strong> Vector Spaces, Subspaces, Span, Linear Independence.</li>
                            <li><strong>Sources:</strong> 
                                <ul>
                                    <li><em>Linear Algebra Done Right</em> by Sheldon Axler.</li>
                                    <li><em>Mathematics for Machine Learning</em> by Deisenroth, Faisal, Ong.</li>
                                </ul>
                            </li>
                            <li><strong>Prerequisites:</strong> High school mathematics, basic Python.</li>
                            <li><strong>Interesting Score:</strong> 7/10.</li>
                            <li><strong>The Insight:</strong> As long as adding and scaling vectors doesn‚Äôt push you outside the set, you‚Äôre in a vector space.</li>
                        </ul>
                    </div>

                    <h3>Concept Summary (With Math + Python Awareness)</h3>
                    
                    <h4>1. Vector Space</h4>
                    <p>A vector space $V$ over a field $\mathbb{F}$ is a set where vector addition and scalar multiplication are defined and satisfy specific axioms. Formally, for all $\vec{u}, \vec{v} \in V$ and $\alpha \in \mathbb{F}$:</p>
                    <p>$$ \vec{u} + \vec{v} \in V \quad \text{and} \quad \alpha \vec{v} \in V $$</p>
                    <p><strong>Python Awareness:</strong> Vector spaces show up as arrays supporting addition and scaling.</p>
<pre>
v = np.array([1, 2, 3])
# Result remains in the same "space" (array of size 3)
result = 2 * v + v  
</pre>

                    <h4>2. Subspace</h4>
                    <p>A subspace $U \subseteq V$ is a subset that is itself a vector space. Instead of checking all axioms, we verify three conditions:</p>
                    <ol>
                        <li>$\vec{0} \in U$ (Contains the zero vector).</li>
                        <li>If $\vec{u}, \vec{v} \in U$, then $\vec{u} + \vec{v} \in U$ (Closed under addition).</li>
                        <li>If $\vec{v} \in U$ and $\alpha \in \mathbb{F}$, then $\alpha \vec{v} \in U$ (Closed under scaling).</li>
                    </ol>
                    <p><strong>Python Awareness:</strong> Subspaces appear when linear combinations ($a \cdot u + b \cdot v$) stay within the same space.</p>

                    <h4>3. Span</h4>
                    <p>Given vectors $\vec{v}_1, \dots, \vec{v}_n$, their span is the set of all reachable vectors:</p>
                    <p>$$ \text{span}(\vec{v}_1, \dots, \vec{v}_n) = \{ \sum_{i=1}^{n} \alpha_i \vec{v}_i \mid \alpha_i \in \mathbb{F} \} $$</p>
                    <p>I‚Äôm starting to think of span as the <strong>boundary of possibility</strong>‚Äîanything outside it is unreachable without new vectors.</p>
                    <p><strong>Python Awareness:</strong> Span corresponds to the column space of a matrix.</p>
<pre>
# Checking the rank (dimension of the span)
rank = np.linalg.matrix_rank(X)
</pre>

                    <h4>4. Linear Independence</h4>
                    <p>A set of vectors is linearly independent if the only way to sum them to zero is by setting all coefficients to zero:</p>
                    <p>$$ \sum \alpha_i \vec{v}_i = \vec{0} \implies \alpha_1 = \dots = \alpha_n = 0 $$</p>
                    <p><strong>Reframed:</strong> If one vector can be built from the others, it isn‚Äôt adding a new direction. Independent vectors expand the span; dependent ones don‚Äôt.</p>
                    <p><strong>Python Awareness:</strong> Independence shows up as full column rank.</p>
<pre>
# If True, columns are independent
is_independent = np.linalg.matrix_rank(X) == X.shape[1]
</pre>

                    <hr>

                    <h3>How This Connects to Data Science</h3>
                    <p>I‚Äôm not a data scientist, but while reading about regression and feature engineering, these ideas kept showing up‚Äîespecially around <strong>multicollinearity</strong>. Once I learned that features are treated as vectors, the connection became clear.</p>

                    <p><strong>Example: Redundant Features</strong><br>
                    Imagine a dataset with two columns:</p>
                    <ul>
                        <li>$\vec{v}_1$: Age in years $\rightarrow [20, 30, 40]$</li>
                        <li>$\vec{v}_2$: Age in months $\rightarrow [240, 360, 480]$</li>
                    </ul>
                    
                    <p>Since $\vec{v}_2 = 12 \cdot \vec{v}_1$, these vectors are <strong>Linearly Dependent</strong>.</p>
                    <ul>
                        <li><strong>Linear Algebra View:</strong> Both generate the same span.</li>
                        <li><strong>Machine Learning View:</strong> One feature adds <em>no new information</em>.</li>
                    </ul>

                    

                    <p>This is exactly what causes <strong>Multicollinearity</strong>‚Äîfeatures that look different but encode the same direction, which can destabilize regression models.</p>

                    <div class="read-more" onclick="togglePost('post-001')">&uarr; Close Log</div>
                </div>
            </div>

        <div class="sidebar">
            <div class="widget">
                <h3>The Editors: Abhinav Rastogi & Stuti Goyal</h3>
                <p>We are MBA students at IIT Madras (Class of 2027) exploring Data Science and the Math behind it.</p>
            </div>
            <div class="widget">
                <h3>Research Interests</h3>
                <ul>
                    <li><a href="#">Linear Algebra</a></li>
                    <li><a href="#">Statistics & Regression</a></li>
                    <li><a href="#">Python for Data Science</a></li>
                </ul>
            </div>
            <div class="widget">
                <h3>Search</h3>
                <input type="text" placeholder="Search..." style="width: 100%; padding: 5px; margin-top: 5px;">
            </div>
        </div>
    </div>
</div>
</body>
</html>


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proof by Pixel</title>

    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script>
        function togglePost(id) {
            var fullContent = document.getElementById(id + '-full');
            // Toggle Logic
            if (fullContent.style.display === "none") {
                fullContent.style.display = "block";
                if (window.MathJax) MathJax.typesetPromise(); 
            } else {
                fullContent.style.display = "none";
            }
        }

        function openNext(nextId) {
            var nextContent = document.getElementById(nextId + '-full');
            if (nextContent.style.display === "none") {
                nextContent.style.display = "block";
                if (window.MathJax) MathJax.typesetPromise(); 
            }
            document.getElementById(nextId).scrollIntoView({behavior: 'smooth'});
        }
    </script>

    <style>
        /* --- GLOBAL RESET --- */
        * { box-sizing: border-box; } 
        body { margin: 0; padding: 0; background-color: #f8f9fa; font-family: "Georgia", "Times New Roman", serif; color: #333; line-height: 1.6; font-size: 16px; overflow-x: hidden; }
        a { color: #0066cc; text-decoration: none; cursor: pointer; }
        a:hover { text-decoration: underline; }

        /* --- COLORS --- */
        :root {
            --brand-color: #2c3e50;
            --link-blue: #0066cc;
        }

        /* --- LAYOUT --- */
        .wrapper { max-width: 960px; width: 96%; margin: 0 auto; background-color: #ffffff; border-left: 1px solid #ddd; border-right: 1px solid #ddd; min-height: 100vh; box-shadow: 0 0 15px rgba(0,0,0,0.03); }
        
        /* --- HEADER --- */
        header { padding: 40px 40px 30px 40px; background-color: var(--brand-color); color: #ffffff; }
        .header-top { display: flex; align-items: center; }
        .logo { background-color: #ffffff; color: var(--brand-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-weight: bold; font-size: 18px; padding: 5px 10px; border-radius: 3px; margin-right: 15px; letter-spacing: -0.5px; }
        header h1 { margin: 0; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px; letter-spacing: -1px; display: inline-block; }
        .tagline { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 14px; color: #cfd9e0; font-style: normal; margin-top: 5px; margin-left: 2px; opacity: 0.9; }

        /* --- NAVIGATION --- */
        nav { padding: 15px 40px; background-color: #ecf0f1; border-bottom: 1px solid #ddd; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 13px; font-weight: bold; text-transform: uppercase; }
        nav a { margin-right: 25px; color: #555; transition: color 0.2s; }
        nav a:hover { color: var(--brand-color); }

        .container { display: flex; padding: 30px 40px; }
        .content { flex: 2; padding-right: 40px; min-width: 0; } 
        .sidebar { flex: 1; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 12px; color: #666; border-left: 1px solid #eee; padding-left: 20px; min-width: 0; }

        /* --- SECTION HEADERS --- */
        .section-label {
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 12px;
            font-weight: bold;
            color: #999;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }

        /* --- POST STYLING --- */
        .post { 
            margin-bottom: 20px;
            border-bottom: 1px solid #f0f0f0;
            padding-bottom: 10px;
        }
        .post:last-child { border-bottom: none; }
        
        .post-header {
            display: flex;
            justify-content: space-between;
            align-items: baseline;
        }

        .post-title { 
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; 
            font-size: 16px; /* Smaller */
            font-weight: 400; /* Non-bold */
            color: var(--link-blue);
            cursor: pointer; 
            margin: 0;
            line-height: 1.4;
        }
        .post-title:hover { text-decoration: underline; color: #003366; }

        .post-date { 
            font-size: 11px; 
            color: #aaa; 
            text-transform: uppercase; 
            letter-spacing: 0.5px; 
            margin: 0;
            white-space: nowrap;
            margin-left: 10px;
        }

        /* Checkpoint Styling (Special) */
        .checkpoint-post .post-title {
            color: #c0392b; /* Reddish for alerts/checkpoints */
            font-weight: bold; 
        }
        .checkpoint-box {
            border: 2px solid #c0392b;
            padding: 20px;
            border-radius: 5px;
            background-color: #fff9f9;
        }

        /* Content Styling */
        h3 { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 18px; margin-top: 30px; border-bottom: 2px solid #333; padding-bottom: 5px; }
        h4 { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 15px; margin-top: 25px; font-weight: bold; color: #444; }
        p { margin-bottom: 1.2em; text-align: left; }
        pre { background:#f4f4f4; padding:15px; border-radius:5px; font-family: "Monaco", "Courier New", monospace; font-size:13px; overflow-x: auto; border: 1px solid #eee; border-left: 3px solid #333; }
        
        /* STUDY LOG META BOX */
        .study-log { background-color: #f9f9f9; padding: 20px; border-left: 3px solid var(--brand-color); margin-bottom: 30px; border-radius: 0 5px 5px 0; margin-top: 20px; }
        .study-log ul { list-style: none; padding: 0; margin: 0; font-size: 14px; color: #555; }
        .study-log li { margin-bottom: 8px; }
        .study-log strong { color: var(--brand-color); }

        /* Hidden by default */
        .post-full { display: none; animation: fadeIn 0.5s; margin-top: 15px; padding-bottom: 20px; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }

        .next-link { margin-top: 30px; padding-top: 20px; border-top: 1px dashed #ddd; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-weight: bold; font-size: 14px; }

        /* Intro Note */
        .intro-note { background-color: #e8f4fc; border-left: 4px solid var(--brand-color); padding: 15px 20px; margin-bottom: 30px; color: #444; font-size: 15px; font-style: italic; border-radius: 0 4px 4px 0; }
        .intro-note strong { color: var(--brand-color); font-style: normal; }

        /* Sidebar Widgets */
        .widget { margin-bottom: 40px; }
        .widget h3 { font-size: 11px; text-transform: uppercase; color: #333; border-bottom: 1px solid #ddd; padding-bottom: 5px; margin-bottom: 10px; margin-top: 0; }
        .widget ul { list-style: none; padding: 0; margin: 0; }
        .widget li { margin-bottom: 8px; }

        @media (max-width: 768px) {
            .container { flex-direction: column; padding: 20px; }
            .sidebar { border-left: none; padding-left: 0; margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }
            .content { padding-right: 0; }
            nav { padding: 15px 20px; }
            header { padding: 30px 20px; }
            .header-top { flex-direction: column; align-items: flex-start; }
            .logo { margin-bottom: 10px; }
        }
    </style>
</head>
<body>

<div class="wrapper">
    <header>
        <div class="header-top">
            <div class="logo">PbyP</div>
            <h1>Proof by Pixel</h1>
        </div>
        <div class="tagline">A data science blog edited by Abhinav Rastogi & Stuti Goyal</div>
    </header>

    <nav>
        <a href="#">Home</a>
        <a href="#">About Us</a>
        <a href="#">Study Logs</a>
        <a href="#">Data Science Problems</a>
    </nav>

    <div class="container">
        <div class="content">

            <div class="intro-note">
                <strong>A note to the reader:</strong> This blog is structured like a learning pipeline. We write study logs while learning the mathematics behind data models‚Äîone concept at a time. After a few logs, we write a checkpoint, where we apply those ideas to a small, hypothetical data science problem.
            </div>

            <div class="section-label">Study Logs</div>
            
            <div class="post" id="post-001">
                <div class="post-header" onclick="togglePost('post-001')">
                    <div class="post-title">Log 001: Understanding the Basis of Data</div>
                    <div class="post-date">16 Dec 2025</div>
                </div>
                <div class="post-full" id="post-001-full">
                    <div class="study-log">
                        <h4 style="margin-top:0;">üìù The Study Log</h4>
                        <ul>
                            <li><strong>Topics:</strong> Vector Spaces, Subspaces, Span, Linear Independence.</li>
                            <li><strong>Sources:</strong> <em>Linear Algebra Done Right</em> (Axler), <em>Mathematics for Machine Learning</em>.</li>
                            <li><strong>Prerequisites:</strong> High school mathematics, basic Python.</li>
                            <li><strong>Interesting Score:</strong> 7/10.</li>
                            <li><strong>The Insight:</strong> As long as adding and scaling vectors doesn‚Äôt push you outside the set, you‚Äôre in a vector space.</li>
                        </ul>
                    </div>
                    <h3>Concept Summary (With Math + Python Awareness)</h3>
                    <h4>1. Vector Space</h4>
                    <p>A vector space $V$ over a field $\mathbb{F}$ is a set where vector addition and scalar multiplication are defined and satisfy specific axioms. Formally, for all $\vec{u}, \vec{v} \in V$ and $\alpha \in \mathbb{F}$:</p>
                    <p>$$ \vec{u} + \vec{v} \in V \quad \text{and} \quad \alpha \vec{v} \in V $$</p>
                    <p><strong>Python Awareness:</strong> Vector spaces show up as arrays supporting addition and scaling.</p>
<pre>
v = np.array([1, 2, 3])
# Result remains in the same "space"
result = 2 * v + v  
</pre>

                    <h4>2. Subspace</h4>
                    <p>A subspace $U \subseteq V$ is a subset that is itself a vector space. Instead of checking all axioms, we verify three conditions:</p>
                    <ol>
                        <li>$\vec{0} \in U$ (Contains the zero vector).</li>
                        <li>If $\vec{u}, \vec{v} \in U$, then $\vec{u} + \vec{v} \in U$ (Closed under addition).</li>
                        <li>If $\vec{v} \in U$ and $\alpha \in \mathbb{F}$, then $\alpha \vec{v} \in U$ (Closed under scaling).</li>
                    </ol>

                    <h4>3. Span</h4>
                    <p>Given vectors $\vec{v}_1, \dots, \vec{v}_n$, their span is the set of all reachable vectors:</p>
                    <p>$$ \text{span}(\vec{v}_1, \dots, \vec{v}_n) = \{ \sum_{i=1}^{n} \alpha_i \vec{v}_i \mid \alpha_i \in \mathbb{F} \} $$</p>
                    <p>I‚Äôm starting to think of span as the <strong>boundary of possibility</strong>‚Äîanything outside it is unreachable without new vectors.</p>
                    <p><strong>Python Awareness:</strong> Span corresponds to the column space of a matrix.</p>
<pre>
rank = np.linalg.matrix_rank(X)
</pre>

                    <h4>4. Linear Independence</h4>
                    <p>A set of vectors is linearly independent if the only way to sum them to zero is by setting all coefficients to zero:</p>
                    <p>$$ \sum \alpha_i \vec{v}_i = \vec{0} \implies \alpha_1 = \dots = \alpha_n = 0 $$</p>
                    <p><strong>Reframed:</strong> If one vector can be built from the others, it isn‚Äôt adding a new direction. Independent vectors expand the span; dependent ones don‚Äôt.</p>

                    <hr>
                    <h3>Data Science Connection: Redundant Features</h3>
                    <p>If two columns in a dataset are linearly dependent (e.g., Age in Years vs Age in Months), one adds no new information to the Span. This causes <strong>Multicollinearity</strong>.</p>
                    
                    <div style="text-align:center; margin: 30px 0; border:1px solid #ddd; padding:10px; background:white;">
                        <svg width="100%" height="150" viewBox="0 0 500 150">
                            <line x1="50" y1="100" x2="450" y2="100" stroke="#eee" />
                            <line x1="50" y1="100" x2="150" y2="100" stroke="blue" stroke-width="3" />
                            <text x="100" y="80" fill="blue" font-size="12">v1</text>
                            <line x1="50" y1="100" x2="350" y2="100" stroke="red" stroke-width="1" stroke-dasharray="5,5" />
                            <text x="300" y="80" fill="red" font-size="12">v2 (Redundant)</text>
                        </svg>
                        <br><em style="font-size:11px; color:#888;">Dependent vectors lie on the same line.</em>
                    </div>
                    <div class="next-link">Next: <a onclick="openNext('post-002')">Log 002 &rarr; Rank & Affine Spaces</a></div>
                </div>
            </div>

            <div class="post" id="post-002">
                <div class="post-header" onclick="togglePost('post-002')">
                    <div class="post-title">Log 002: Rank, Linear Mappings & Affine Spaces</div>
                    <div class="post-date">17 Dec 2025</div>
                </div>
                <div class="post-full" id="post-002-full">
                    <div class="study-log">
                        <h4 style="margin-top:0;">üìù The Study Log</h4>
                        <ul>
                            <li><strong>Topics:</strong> Rank, Linear Mappings, Kernel/Image, Affine Spaces.</li>
                            <li><strong>Sources:</strong> <em>Linear Algebra Done Right</em> (Axler), <em>Mathematics for Machine Learning</em>.</li>
                            <li><strong>Interesting Score:</strong> 8/10.</li>
                            <li><strong>The Insight:</strong> Information is either preserved (Rank) or lost (Kernel), never both.</li>
                        </ul>
                    </div>
                    <h3>Concept Summary</h3>
                    <h4>1. Rank</h4>
                    <p>Rank measures the "real" dimension of data. $\text{rank}(A) = \dim(\text{Col}(A))$.</p>
                    <p><strong>Python Awareness:</strong> <code>np.linalg.matrix_rank(X)</code></p>

                    <h4>2. Linear Mapping ($T$)</h4>
                    <p>A function $T: V \to W$ is linear if it preserves structure (respects addition and scaling). It does not bend or shift space.</p>
                    <p>$$ T(\vec{x}) = A\vec{x} $$</p>

                    <h4>3. Affine Spaces</h4>
                    <p>An affine space is a vector space shifted away from the origin: $\mathcal{A} = \vec{v}_0 + U$.</p>

                    <hr>
                    <h3>Data Science Connection: Bias Terms</h3>
                    <p>A <strong>Linear</strong> model ($y=Ax$) must pass through zero. An <strong>Affine</strong> model ($y=Ax+b$) can shift to fit data anywhere.</p>
                    
                    <div style="text-align:center; margin: 30px 0; border:1px solid #ddd; padding:10px; background:white;">
                        <svg width="100%" height="200" viewBox="0 0 400 200">
                             <g stroke="#eee" stroke-width="1"><line x1="200" y1="10" x2="200" y2="190" /><line x1="10" y1="150" x2="390" y2="150" /></g>
                            <line x1="100" y1="200" x2="300" y2="100" stroke="blue" stroke-width="2" />
                            <text x="310" y="100" fill="blue" font-size="12">Linear</text>
                            <line x1="100" y1="100" x2="300" y2="20" stroke="red" stroke-width="2" />
                            <text x="310" y="20" fill="red" font-size="12">Affine</text>
                            <circle cx="200" cy="150" r="3" fill="#333" />
                        </svg>
                    </div>
                    <div class="next-link">Next: <a onclick="openNext('post-003')">Log 003 &rarr; Norms & Inner Products</a></div>
                </div>
            </div>

            <div class="post" id="post-003">
                <div class="post-header" onclick="togglePost('post-003')">
                    <div class="post-title">Log 003: Norms & Inner Products</div>
                    <div class="post-date">18 Dec 2025</div>
                </div>
                <div class="post-full" id="post-003-full">
                    <div class="study-log">
                        <h4 style="margin-top:0;">üìù The Study Log</h4>
                        <ul>
                            <li><strong>Topics:</strong> Norms, Inner Products, Length, Angle.</li>
                            <li><strong>Sources:</strong> <em>Linear Algebra Done Right</em> (Axler).</li>
                            <li><strong>Interesting Score:</strong> 8/10.</li>
                            <li><strong>The Insight:</strong> Inner products allow vectors to "talk" to each other (measure alignment).</li>
                        </ul>
                    </div>
                    <h3>Concept Summary</h3>
                    <h4>1. Norm vs. Inner Product</h4>
                    <p><strong>Norm:</strong> Measures length ($||\vec{v}||$).<br>
                    <strong>Inner Product:</strong> Measures interaction/angle ($\langle \vec{u}, \vec{v} \rangle$).</p>
                    <p>$$ \cos(\theta) = \frac{\langle \vec{u}, \vec{v} \rangle}{\|\vec{u}\| \|\vec{v}\|} $$</p>
                    
                    <h4>2. Distance (Derived from Norm)</h4>
                    <p>Once a norm exists, it defines a distance. Distance is just the length of the difference vector.</p>
                    <p>$$ d(\vec{u}, \vec{v}) = \|\vec{u} - \vec{v}\| $$</p>

                    <hr>
                    <h3>Data Science Connection</h3>
                    <p><strong>Cosine Similarity</strong> uses the inner product (ignoring magnitude), useful for text analysis. <strong>Euclidean Distance</strong> uses the norm, useful when magnitude matters (e.g., salary data).</p>
                    <div class="next-link">Next: <a onclick="openNext('post-004')">Log 004 &rarr; Orthogonality</a></div>
                </div>
            </div>

            <div class="post" id="post-004">
                <div class="post-header" onclick="togglePost('post-004')">
                    <div class="post-title">Log 004: Angles, Orthogonality & Structure</div>
                    <div class="post-date">19 Dec 2025</div>
                </div>
                <div class="post-full" id="post-004-full">
                    <div class="study-log">
                        <h4 style="margin-top:0;">üìù The Study Log</h4>
                        <ul>
                            <li><strong>Topics:</strong> Orthogonality, Orthogonal Complements ($U^\perp$).</li>
                            <li><strong>Insight:</strong> Orthogonality means independence. "Least Squares" is geometric projection.</li>
                        </ul>
                    </div>
                    <h3>Concept Summary</h3>
                    <h4>1. Orthogonality</h4>
                    <p>Vectors are orthogonal if $\langle \vec{u}, \vec{v} \rangle = 0$. This implies they share no directional information.</p>
                    <p><strong>Python Awareness:</strong> <code>np.dot(u, v) == 0</code></p>

                    <h4>2. Orthogonal Decomposition</h4>
                    <p>Any vector can be split into a "Signal" part (in subspace $U$) and a "Noise" part (in $U^\perp$).</p>

                    <hr>
                    <h3>Data Science Connection: Least Squares</h3>
                    <p>In regression, the "Error" is the part of the target vector that lives in the <strong>Orthogonal Complement</strong> of the feature space.</p>
                    
                    <div style="text-align:center; margin: 30px 0; border:1px solid #ddd; padding:10px; background:white;">
                        <svg width="100%" height="200" viewBox="0 0 400 200">
                            <polygon points="50,150 150,180 350,130 250,100" fill="#eef" stroke="#99a" />
                            <text x="300" y="120" fill="#77a">Feature Space</text>
                            <line x1="150" y1="180" x2="150" y2="20" stroke="#333" stroke-width="2" />
                            <text x="140" y="20" fill="#333">y (Target)</text>
                            <line x1="150" y1="180" x2="250" y2="140" stroke="blue" stroke-width="3" />
                            <text x="260" y="150" fill="blue">Prediction</text>
                            <line x1="250" y1="140" x2="150" y2="20" stroke="red" stroke-width="2" stroke-dasharray="5,5" />
                            <text x="210" y="80" fill="red">Error</text>
                        </svg>
                        <br><em style="font-size:11px; color:#888;">The error is orthogonal to the prediction.</em>
                    </div>
                    <div class="next-link">Next: <a onclick="openNext('checkpoint-001')">Checkpoint 001 &rarr; The Geometry of Linear Regression</a></div>
                </div>
            </div>


            <div class="section-label" style="margin-top: 50px;">Data Science Problems</div>

            <div class="post checkpoint-post" id="checkpoint-001">
                <div class="post-header" onclick="togglePost('checkpoint-001')">
                    <div class="post-title">Checkpoint 001: The Geometry of Linear Regression</div>
                    <div class="post-date" style="color:#c0392b;">20 Dec 2025</div>
                </div>

                <div class="post-full" id="checkpoint-001-full">
                    
                    <div class="checkpoint-box">
                        <h4 style="margin-top:0; color:#c0392b;">üéØ The Problem</h4>
                        <p><strong>Goal:</strong> Predict House Prices ($y$) based on Size and Age ($X$).<br>
                        <strong>The Issue:</strong> Real-world data is noisy. The points don't form a perfect line. There is no solution to $X\vec{w} = \vec{y}$.</p>
                    </div>

                    <h3>Applying The Logs</h3>
                    
                    <p>Instead of thinking of this as "minimizing error", we use the geometry we learned in Logs 001-004.</p>

                    <ul>
                        <li><strong>Log 001 (Span):</strong> Our features (Size, Age) create a "Plane of Possibility" (the Span). The target vector $\vec{y}$ (Prices) floats <em>above</em> this plane.</li>
                        <li><strong>Log 002 (Rank):</strong> If Size and Age are correlated, the Rank drops, and our plane collapses into a line.</li>
                        <li><strong>Log 003 (Distance):</strong> We want the $\vec{w}$ that makes the distance $||\vec{y} - X\vec{w}||$ as small as possible.</li>
                        <li><strong>Log 004 (Orthogonality):</strong> The shortest path from $\vec{y}$ to the plane is a straight line down. The Error vector must be <strong>Orthogonal</strong> to the plane.</li>
                    </ul>

                    <div style="text-align:center; margin: 30px 0; border:1px solid #eee; padding:20px; background:white;">
                        <svg width="100%" height="200" viewBox="0 0 400 200">
                            <polygon points="50,150 150,180 350,130 250,100" fill="#fdf2f2" stroke="#e6b0aa" stroke-width="2"/>
                            <text x="270" y="125" fill="#c0392b" font-size="12">Feature Span (Logs 1-2)</text>
                            
                            <line x1="150" y1="180" x2="150" y2="40" stroke="#333" stroke-width="2"/>
                            <text x="130" y="35" fill="#333" font-weight="bold">y (Price)</text>
                            
                            <line x1="150" y1="180" x2="250" y2="140" stroke="#c0392b" stroke-width="3"/>
                            <text x="240" y="160" fill="#c0392b" font-weight="bold">Prediction</text>
                            
                            <line x1="250" y1="140" x2="150" y2="40" stroke="#999" stroke-width="2" stroke-dasharray="5,5"/>
                            <text x="210" y="80" fill="#999">Error (Log 4)</text>
                        </svg>
                        <br><em style="font-size:11px; color:#666;">Figure 1: Applying all 4 concepts to solve Regression.</em>
                    </div>

                    <p><strong>The Solution:</strong> Because of Log 004, we know $\langle X\vec{w}, \vec{y} - X\vec{w} \rangle = 0$. This leads directly to the Normal Equation:</p>
                    <p>$$ \vec{w} = (X^\top X)^{-1} X^\top \vec{y} $$</p>

                    <div class="next-link">&uarr; Close Checkpoint</div>
                </div>
            </div>

        </div>

        <div class="sidebar">
            <div class="widget">
                <h3>The Editors: Abhinav Rastogi & Stuti Goyal</h3>
                <p>We are MBA students at IIT Madras (Class of 2027) exploring Data Science and the Math behind it.</p>
            </div>
            <div class="widget">
                <h3>Research Interests</h3>
                <ul>
                    <li><a href="#">Linear Algebra</a></li>
                    <li><a href="#">Statistics & Regression</a></li>
                    <li><a href="#">Python for Data Science</a></li>
                </ul>
            </div>
            <div class="widget">
                <h3>Search</h3>
                <input type="text" placeholder="Search..." style="width: 100%; padding: 5px; margin-top: 5px;">
            </div>
        </div>
    </div>
</div>
</body>
</html>
